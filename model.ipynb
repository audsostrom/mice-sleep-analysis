{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draft Model for Mice Steep Stage Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import tkinter as tk #tk for file dialog (requires Jinja2!!!)\n",
    "from tkinter import filedialog #tkinter for file dialog\n",
    "\n",
    "import re #regex for parsing\n",
    "from os.path import exists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd import the data and create a dataframe out of it. (SHOUTOUT REGAN!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTextFile(filepath):\n",
    "\tfLen = len(filepath)\n",
    "\t#Check that our filepath is a .txt\n",
    "\treturn bool((fLen > 4) and (filepath[fLen-4:] == \".txt\"))\n",
    "\n",
    "#any data that doesn't make a full epoch gets cut off im pretty sure (because the epoch tensor is size INT(maxSize/epoch_size))\n",
    "def getTensorFromText(filepath, maxSize=None, epoch_size = 100):## just change the \n",
    "\t#Check that our filepath is a .txt\")\n",
    "    if (isTextFile(filepath)):\n",
    "        dataFp = open(filepath, \"r\")\n",
    "        readingData = False\n",
    "        col1, col2, col3,  col4 = [], [], [], []\n",
    "\n",
    "        line_count = 0\n",
    "\n",
    "        epoch_tensor = torch.zeros(int(maxSize/epoch_size), 3, epoch_size)#tensor to hold each epoch of data\n",
    "        epoch_start = 0\n",
    "        epoch_end = epoch_size\n",
    "        epoch_index = 0\n",
    "\n",
    "        for line in dataFp.readlines():\n",
    "            if ((maxSize != None) and (maxSize <= line_count)):\n",
    "                break\n",
    "\n",
    "            if epoch_start >= epoch_end:\n",
    "                epoch = torch.Tensor([col1, col2, col3])#im just getting one of the time series columns for now for simplicities sake\n",
    "                epoch_tensor[epoch_index] = epoch\n",
    "                col1, col2, col3, col4 = [], [], [], []                \n",
    "                epoch_end += epoch_size\n",
    "                epoch_index += 1\n",
    "\n",
    "            if readingData:                                        \n",
    "\t\t\t\t#split the line by tabs and add the data to our column lists\n",
    "                data_list = line.split(\"\\t\")\n",
    "                stripped_time = re.sub(\"\\s\", \"\",data_list[1])\n",
    "                time = stripped_time.split(\":\")\n",
    "\n",
    "                col1.append(int(time[0]))#idk how much it matters for storage but it might be easier to multiply this by 60\n",
    "                col2.append(np.float32(time[1]))#i think we do need this to be precise because the timesteps should be consitent so i changed it to 32\n",
    "                col3.append(np.float16(re.sub(\"\\s\", \"\",data_list[2])))\n",
    "                col4.append(np.float16(re.sub(\"\\s\", \"\",data_list[3])))\n",
    "\n",
    "                line_count += 1\n",
    "                epoch_start += 1\n",
    "                \n",
    "\t\t\t\t#Ignore initial lines\n",
    "            else:\t\t\n",
    "                stripped = re.sub(\"\\s\", \"\", line)\n",
    "\t\t\t\t\t#This line is the last line before data comes\n",
    "                if stripped == \"(m:s.ms)(mV)(mV)\":\n",
    "                    readingData = True\n",
    "\n",
    "        dataFp.close()\n",
    "\n",
    "        return epoch_tensor\n",
    "    else:\n",
    "        print(\"Cannot handle file\", filepath)\n",
    "        return None\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_tensor = getTensorFromText(\"../CHDCtrl1_CHD801FR_normal/CHD801FR_20221123_normal.txt\", maxSize =1000000, epoch_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_df = pd.DataFrame(eeg_tensor[:, :, 0]).astype(\"float\")\n",
    "end_df = pd.DataFrame(eeg_tensor[:, :, 199]).astype(\"float\")\n",
    "\n",
    "eeg_df = eeg_df.rename(columns={0: \"min\", 1: \"sec\", 2:\"sig\"})\n",
    "eeg_df[\"min\"] = (eeg_df[\"min\"]*60) + eeg_df.sec\n",
    "\n",
    "end_df = end_df.rename(columns={0: \"min\", 1: \"sec\", 2:\"sig\"})\n",
    "end_df[\"sec\"] = (end_df[\"min\"]*60) + end_df.sec\n",
    "\n",
    "eeg_df[\"sec\"] = end_df[\"sec\"]\n",
    "eeg_df = eeg_df.rename(columns={\"min\": \"start\", \"sec\": \"end\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStatesOverTime(filepath):## just change the \n",
    "\t#Check that our filepath is a .txt\")\n",
    "    time, state = [], []\n",
    "    \n",
    "    data_an = open(filepath)#open and read file\n",
    "    r = data_an.read()\n",
    "    data_an.close()\n",
    "\n",
    "    rows = list(r.split(\"\\n\"))#list of each row (strings)\n",
    "    for row in rows:\n",
    "        row_list = list(row.split(\",\"))\n",
    "        if row_list == [\"\"]:\n",
    "            break\n",
    "\n",
    "        time.append(np.float32(row_list[2]))\n",
    "        state.append(int(row_list[5]))\n",
    "\n",
    "    return [time, state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "statesOverTime = getStatesOverTime(\"../CHDCtrl1_CHD801FR_normal/CHD801FR_20221123_normal_annotated.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_column(s = 0, e = 10000):\n",
    "\n",
    "    states_col = []\n",
    "    epoch_i = 1\n",
    "    time_state_i = 1\n",
    "\n",
    "    for index, row in eeg_df[s:e].iterrows():\n",
    "        epoch_start = row[\"start\"]\n",
    "        epoch_end = row[\"end\"]\n",
    "\n",
    "        if epoch_end >= statesOverTime[0][time_state_i]:\n",
    "            \n",
    "            if epoch_start < statesOverTime[0][time_state_i]:\n",
    "                states_col.append(4)\n",
    "\n",
    "            else:\n",
    "                states_col.append(statesOverTime[1][time_state_i])\n",
    "            time_state_i += 1\n",
    "\n",
    "        else:\n",
    "            states_col.append(statesOverTime[1][time_state_i])\n",
    "\n",
    "        epoch_i += 1\n",
    "  \n",
    "    return(states_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_column(s = 0, e = 10000):\n",
    "\n",
    "    states_col = []\n",
    "    epoch_i = 1\n",
    "    time_state_i = 1\n",
    "\n",
    "    for index, row in eeg_df[s:e].iterrows():\n",
    "        epoch_start = row[\"start\"]\n",
    "        epoch_end = row[\"end\"]\n",
    "\n",
    "        if epoch_end >= statesOverTime[0][time_state_i]:\n",
    "            \n",
    "            if epoch_start < statesOverTime[0][time_state_i]:\n",
    "                states_col.append(4)\n",
    "\n",
    "            else:\n",
    "                states_col.append(statesOverTime[1][time_state_i])\n",
    "            time_state_i += 1\n",
    "\n",
    "        else:\n",
    "            states_col.append(statesOverTime[1][time_state_i])\n",
    "\n",
    "        epoch_i += 1\n",
    "  \n",
    "    return(states_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfigure out how to integrate this\\n\\n# set up training and validation data\\ntrain_data, train_labels, val_data, val_labels = # USE DATAFRAME TO MAKE THIS\\n\\ntrain = \\ntrain_loader = utils.DataLoader(train, batch_size=64, shuffle=True)\\n\\ntest = \\ntest_loader = utils.DataLoader(test, batch_size=64, shuffle=True)\\n\\ndata_loaders = {'train': train_loader, 'valid': test_loader}\\ndataset_sizes = {'train': len(train), 'valid': len(test)}\\n\""
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = add_state_column()\n",
    "ds = torch.utils.data.TensorDataset(eeg_tensor, torch.tensor(labels))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True)\n",
    "\n",
    "\"\"\"\n",
    "figure out how to integrate this\n",
    "\n",
    "# set up training and validation data\n",
    "train_data, train_labels, val_data, val_labels = # USE DATAFRAME TO MAKE THIS\n",
    "\n",
    "train = \n",
    "train_loader = utils.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "test = \n",
    "test_loader = utils.DataLoader(test, batch_size=64, shuffle=True)\n",
    "\n",
    "data_loaders = {'train': train_loader, 'valid': test_loader}\n",
    "dataset_sizes = {'train': len(train), 'valid': len(test)}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes for classification tasks (what sleep stage the mouse is in)\n",
    "# based on labels from annotated data, W is wake, N is Non-REM, R is REM, and A is artifact (unique to our model)\n",
    "\n",
    "classes = {0: \"W\", 1: \"N\", 2: \"R\", 3: \"A\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating class for Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" the left hand side of the CNN\"\"\"\n",
    "def CNN_eeg_layer1(fs): \n",
    "    return nn.Sequential(\n",
    "            nn.Conv1d(3, 64, kernel_size=fs//2, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=8, stride=8),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "\"\"\" the right hand side of the CNN\"\"\"\n",
    "def CNN_eeg_layer2(fs): \n",
    "    return nn.Sequential(\n",
    "            # 1 input channel for EEG, 64 filters applied \n",
    "            nn.Conv1d(3, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_cnn_dense=256, fs=10, num_classes=4):\n",
    "      \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        \"\"\" left and right hand sides of CNNs \"\"\"\n",
    "        self.layer1_eeg = CNN_eeg_layer1(fs)        \n",
    "        self.layer2_eeg = CNN_eeg_layer2(fs)\n",
    "\n",
    "         # maybe another network at some point for emg?\n",
    "        \n",
    "        # the fully connected layer concatenating the two outputs\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64, n_cnn_dense),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool1d(kernel_size=4, stride=4))\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(49, num_classes),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "                \n",
    "        \n",
    "    def forward(self, channels):\n",
    "      \n",
    "        # at some point, we'll have a second channel for emg\n",
    "        \n",
    "        ch1 = channels # extract eeg channel (Channel 1) from data frame\n",
    "        # ch2 = # extract emg channel (Channel 2) from data frame\n",
    "        \n",
    "        out1_eeg = self.layer1_eeg(ch1)\n",
    "        print(out1_eeg.shape)\n",
    "        out2_eeg = self.layer2_eeg(ch1)\n",
    "        print(out2_eeg.shape)\n",
    "               \n",
    "        # \n",
    "        out = torch.cat((out1_eeg, out2_eeg), dim=1)\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "        print(out.shape)\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Model**\n",
    "\n",
    "We can play around with the parameters a bit. I've included the function for counting the number of parameters from the 06-convnet.ipynb from the CSE144 example repo.\n",
    "\n",
    "For our optimization, I used Adam because it converges faster and I don't think our data is well-formatted enough yet to get decent results with SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 191880\n"
     ]
    }
   ],
   "source": [
    "# make our model\n",
    "model = CNN()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# define optimization function and print number of params\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one example I saw, they used [model.state_dict()](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html) to record the best learnable parameters (i.e. weights and biases) of a model. They use it for storing the best possible model found during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, num_epochs):\n",
    "    \n",
    "    # deep copy and save the best model weights found\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # initialize best accuracy found to 0\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "                \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader.dataset:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # only compute gradients during training, not\n",
    "            # necessary in validations\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = torch.nn.functional.nll_loss(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "            print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "            if epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "    print('Best Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore this, we'll add it back when we get validation data\n",
    "\"\"\"\n",
    "def train_model(model, criteria, optimizer, scheduler, num_epochs):\n",
    "    \n",
    "    # deep copy and save the best model weights found\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # initialize best accuracy found to 0\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()   \n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # only compute gradients during training, not\n",
    "                # necessary in validations\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criteria(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 49])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x49 and 64x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[413], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sched \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_model(model, optimizer,sched, \u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[412], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m \u001b[39m# only compute gradients during training, not\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# necessary in validations\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     24\u001b[0m _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnll_loss(outputs, labels)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[409], line 71\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, channels)\u001b[0m\n\u001b[1;32m     69\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((out1_eeg, out2_eeg), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 71\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(out)\n\u001b[1;32m     72\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     73\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(out, start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x49 and 64x256)"
     ]
    }
   ],
   "source": [
    "\n",
    "sched = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "train_model(model, optimizer,sched, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
