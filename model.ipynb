{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draft Model for Mice Steep Stage Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd import the data and create a dataframe out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Insert some code here for loading the data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes for classification tasks (what sleep stage the mouse is in)\n",
    "# based on labels from annotated data, W is wake, N is Non-REM, R is REM, and A is artifact (unique to our model)\n",
    "\n",
    "classes = {0: \"W\", 1: \"N\", 2: \"R\", 3: \"A\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating class for Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" the left hand side of the CNN\"\"\"\n",
    "def CNN_eeg_layer1(fs): \n",
    "    return nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=fs//2, stride=fs//16, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=8, stride=8),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "\"\"\" the right hand side of the CNN\"\"\"\n",
    "def CNN_eeg_layer2(fs): \n",
    "    return nn.Sequential(\n",
    "            # 1 input channel for EEG, 64 filters applied \n",
    "            nn.Conv1d(1, 64, kernel_size=fs*4, stride=fs//2, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_cnn_dense=256, fs=100, num_classes=4):\n",
    "      \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        \"\"\" left and right hand sides of CNNs \"\"\"\n",
    "        self.layer1_eeg = CNN_eeg_layer1(fs)        \n",
    "        self.layer2_eeg = CNN_eeg_layer2(fs)\n",
    "\n",
    "         # maybe another network at some point for emg?\n",
    "        \n",
    "        # the fully connected layer concatenating the two outputs\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128, n_cnn_dense),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool1d(kernel_size=4, stride=4))\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(2048, num_classes),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "                \n",
    "        \n",
    "    def forward(self, channels):\n",
    "      \n",
    "        # at some point, we'll have a second channel for emg\n",
    "        \n",
    "        ch1 = # extract eeg channel (Channel 1) from data frame\n",
    "        \n",
    "        out1_eeg = self.layer1_eeg(ch1)\n",
    "        out2_eeg = self.layer2_eeg(ch1)\n",
    "               \n",
    "        \n",
    "        out = torch.cat((out1_eeg, out2_eeg,), dim=1)\n",
    "        out = self.fc1(out)\n",
    "        s = out.size()[0]\n",
    "        out = out.view(s, -1)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Model**\n",
    "\n",
    "We can play around with the parameters a bit. I've included the function for counting the number of parameters from the 06-convnet.ipynb from the CSE144 example repo.\n",
    "\n",
    "For our optimization, I used Adam because it converges faster and I don't think our data is well-formatted enough yet to get decent results with SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make our model\n",
    "model = CNN()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# set up training and validation data\n",
    "train_data, train_labels, val_data, val_labels = # USE DATAFRAME TO MAKE THIS\n",
    "\n",
    "train = # utils.TensorDataset(...)\n",
    "train_loader = utils.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "test = # utils.TensorDataset(...)\n",
    "test_loader = utils.DataLoader(test, batch_size=64, shuffle=True)\n",
    "\n",
    "data_loaders = {'train': train_loader, 'valid': test_loader}\n",
    "dataset_sizes = {'train': len(train), 'valid': len(test)}\n",
    "\n",
    "# define optimization function and print number of params\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one example I saw, they used [model.state_dict()](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html) to record the best learnable parameters (i.e. weights and biases) of a model. They use it for storing the best possible model found during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criteria, optimizer, scheduler, num_epochs):\n",
    "    # deep copy and save the best model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()   \n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criteria(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
