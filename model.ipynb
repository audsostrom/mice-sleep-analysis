{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draft Model for Mice Steep Stage Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import tkinter as tk #tk for file dialog (requires Jinja2!!!)\n",
    "from tkinter import filedialog #tkinter for file dialog\n",
    "\n",
    "import re #regex for parsing\n",
    "from os.path import exists\n",
    "from inputMassager import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd import the data and create a dataframe out of it. (SHOUTOUT REGAN!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputHandler = inputMassager()\n",
    "\n",
    "filepath = inputHandler.askForInput(\"Blah!\")\n",
    "\n",
    "#Period Size Variable : effects CNN architecture\n",
    "periodSize = 200\n",
    "\n",
    "#makePeriodFromTxt(self, filepath, periodSize, maxPeriods=None):\n",
    "df = inputHandler.makePeriodFromTxt(filepath, periodSize, 2000)\n",
    "\n",
    "# target = regan and audreys code ---> This is what will go into the data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time we're on: 1703.0699462890625\n",
      "my time is 1, current epoch is (1.004,2.004)\n",
      "time we're on: 1707.9849853515625\n",
      "my time is 2, current epoch is (1703.441,1704.441)\n",
      "time we're on: 1721.60498046875\n",
      "my time is 3, current epoch is (1708.466,1709.466)\n",
      "time we're on: 1728.739990234375\n",
      "my time is 4, current epoch is (1722.536,1723.536)\n",
      "time we're on: 1732.344970703125\n",
      "my time is 5, current epoch is (1729.571,1730.571)\n",
      "time we're on: 1743.1099853515625\n",
      "my time is 6, current epoch is (1732.586,1733.586)\n",
      "time we're on: 2087.1650390625\n",
      "my time is 7, current epoch is (1743.641,1744.641)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfigure out how to integrate this\\n\\n# set up training and validation data\\ntrain_data, train_labels, val_data, val_labels = # USE DATAFRAME TO MAKE THIS\\n\\ntrain = \\ntrain_loader = utils.DataLoader(train, batch_size=64, shuffle=True)\\n\\ntest = \\ntest_loader = utils.DataLoader(test, batch_size=64, shuffle=True)\\n\\ndata_loaders = {'train': train_loader, 'valid': test_loader}\\ndataset_sizes = {'train': len(train), 'valid': len(test)}\\n\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath2 = inputHandler.askForInput(\"Blah!\")\n",
    "timestamps = find_time_labels(df, filepath2)\n",
    "labels = label_dataframe(df, timestamps)\n",
    "\n",
    "labels\n",
    "\"\"\"\n",
    "figure out how to integrate this\n",
    "\n",
    "# set up training and validation data\n",
    "train_data, train_labels, val_data, val_labels = # USE DATAFRAME TO MAKE THIS\n",
    "\n",
    "train = \n",
    "train_loader = utils.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "test = \n",
    "test_loader = utils.DataLoader(test, batch_size=64, shuffle=True)\n",
    "\n",
    "data_loaders = {'train': train_loader, 'valid': test_loader}\n",
    "dataset_sizes = {'train': len(train), 'valid': len(test)}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start    end                                                 c1   \n",
      "0  0.000  0.999  [-0.04, 0.23, 0.2, 0.29, 0.23, 0.22, 0.25, 0.2...  \\\n",
      "1  1.004  2.004  [0.0, 0.13, 0.03, -0.01, -0.02, -0.02, -0.02, ...   \n",
      "2  2.009  3.009  [-0.02, -0.08, -0.18, -0.13, -0.05, -0.08, -0....   \n",
      "3  3.014  4.014  [0.06, 0.04, 0.04, 0.06, 0.08, 0.06, 0.03, 0.0...   \n",
      "4  4.019  5.019  [0.03, 0.11, 0.22, 0.11, 0.11, 0.02, 0.06, 0.0...   \n",
      "\n",
      "                                                  c2  labels  \n",
      "0  [0.2, -0.14, -0.63, -0.44, 0.13, 0.0, 0.2, -0....       4  \n",
      "1  [0.09, 0.04, 0.61, 0.83, 0.16, 0.12, 0.31, 0.6...       2  \n",
      "2  [-0.15, -0.02, 0.16, -0.26, -0.35, 0.18, -0.19...       2  \n",
      "3  [-0.14, 0.16, -0.12, 0.03, -0.11, 0.08, -0.03,...       2  \n",
      "4  [0.37, -0.21, 0.25, -0.3, -0.16, 0.24, -0.58, ...       2  \n",
      "<torch.utils.data.dataloader.DataLoader object at 0x20c691dc0>\n"
     ]
    }
   ],
   "source": [
    "df['labels'] = labels\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(df, batch_size=64, shuffle=False)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes for classification tasks (what sleep stage the mouse is in)\n",
    "# based on labels from annotated data, W is wake, N is Non-REM, R is REM, and A is artifact (unique to our model)\n",
    "\n",
    "classes = {0: \"W\", 1: \"N\", 2: \"R\", 3: \"A\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating class for Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" the left hand side of the CNN\"\"\"\n",
    "def CNN_eeg_layer1(fs): \n",
    "    return nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=fs//2, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=8, stride=8),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  \n",
    "    def __init__(self, period_size, num_classes=4):\n",
    "      \n",
    "        super(CNN, self).__init__()\n",
    "        self.period_size = period_size\n",
    "        self.conv1 = nn.Conv1d(in_channels=period_size, out_channels=64, kernel_size=period_size//2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=period_size//2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=period_size, out_channels=64, kernel_size=period_size//2)\n",
    "        self.conv4 = nn.Conv1d(in_channels=period_size, out_channels=1048, kernel_size=period_size//2)\n",
    "        self.fc1 = nn.Linear(1048, 50)\n",
    "        self.fc2 = nn.Linear(50, 4)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "                \n",
    "        \n",
    "    def forward(self, channels):\n",
    "      \n",
    "        # at some point, we'll have a second channel for emg\n",
    "        \n",
    "        x = channels # extract eeg channel (Channel 1) from data frame\n",
    "        # ch2 = # extract emg channel (Channel 2) from data frame\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # 4 layers in total\n",
    "        x = x.view(-1, 128*2*2)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.nn.functional.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Model**\n",
    "\n",
    "We can play around with the parameters a bit. I've included the function for counting the number of parameters from the 06-convnet.ipynb from the CSE144 example repo.\n",
    "\n",
    "For our optimization, I used Adam because it converges faster and I don't think our data is well-formatted enough yet to get decent results with SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 23989894\n"
     ]
    }
   ],
   "source": [
    "# make our model\n",
    "model = CNN(period_size=200)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# define optimization function and print number of params\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one example I saw, they used [model.state_dict()](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html) to record the best learnable parameters (i.e. weights and biases) of a model. They use it for storing the best possible model found during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, num_epochs):\n",
    "    \n",
    "    # deep copy and save the best model weights found\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # initialize best accuracy found to 0\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "                \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for index, (inputs, labels) in enumerate(zip(train_loader.dataset['c1'], train_loader.dataset['c2'])):\n",
    "            print(inputs)\n",
    "            inputs = torch.tensor(inputs).to(device)\n",
    "            print(inputs.size)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # only compute gradients during training, not\n",
    "            # necessary in validations\n",
    "            outputs = model(torch.tensor(inputs).view(1, 200))\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = torch.nn.functional.nll_loss(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "            print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "            if epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "    print('Best Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "[-0.04, 0.23, 0.2, 0.29, 0.23, 0.22, 0.25, 0.21, 0.24, 0.21, 0.15, 0.22, 0.16, 0.14, 0.19, 0.11, 0.14, 0.17, 0.19, 0.16, 0.19, 0.21, 0.22, 0.28, 0.23, 0.23, 0.29, 0.22, 0.14, 0.12, 0.06, 0.15, 0.2, 0.16, 0.16, 0.14, 0.05, 0.08, 0.08, 0.1, 0.09, 0.06, 0.1, 0.11, 0.08, 0.0, 0.04, 0.04, 0.09, 0.12, 0.17, 0.18, 0.17, 0.19, 0.2, 0.2, 0.22, 0.17, 0.15, 0.02, 0.01, 0.04, 0.04, 0.06, 0.04, 0.02, -0.0, 0.01, 0.03, 0.03, -0.0, 0.08, 0.05, 0.04, 0.03, -0.01, 0.04, 0.1, 0.12, 0.09, 0.13, 0.14, 0.09, 0.11, 0.01, -0.07, -0.05, -0.05, -0.01, -0.02, -0.1, -0.07, -0.09, -0.08, -0.09, -0.11, -0.11, -0.07, -0.09, -0.06, -0.04, -0.05, -0.1, -0.07, -0.13, -0.1, -0.11, -0.09, -0.03, -0.05, -0.08, -0.05, -0.14, -0.06, -0.12, -0.14, -0.17, -0.14, -0.15, -0.07, -0.09, -0.04, 0.02, -0.02, 0.04, 0.02, -0.01, -0.03, 0.01, 0.01, 0.01, -0.02, -0.08, -0.09, -0.09, -0.11, -0.09, -0.09, -0.11, -0.04, -0.09, -0.05, -0.07, -0.05, -0.08, -0.02, -0.01, -0.05, -0.13, 0.04, -0.04, 0.0, -0.08, -0.07, -0.07, -0.1, -0.01, -0.03, -0.07, -0.03, -0.12, -0.13, -0.18, -0.2, -0.14, -0.12, -0.06, -0.08, 0.02, -0.07, -0.13, -0.07, -0.07, -0.02, 0.07, 0.02, 0.0, -0.03, 0.01, -0.05, -0.03, 0.07, -0.09, -0.05, -0.1, -0.09, -0.1, -0.06, -0.03, -0.09, -0.08, -0.03, -0.03, -0.02, 0.05, 0.02, 0.02, 0.04, 0.02, 0.07]\n",
      "<built-in method size of Tensor object at 0x20e1cd310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dg/6gzrkj09311g91kn90n1qdcc0000gn/T/ipykernel_1860/1889929946.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = model(torch.tensor(inputs).view(1, 200))\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'x' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sched \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_model(model, optimizer,sched, \u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[107], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m \u001b[39m# only compute gradients during training, not\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# necessary in validations\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39;49mtensor(inputs)\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, \u001b[39m200\u001b[39;49m))\n\u001b[1;32m     26\u001b[0m _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnll_loss(outputs, labels)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[104], line 40\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, channels)\u001b[0m\n\u001b[1;32m     37\u001b[0m ch1 \u001b[39m=\u001b[39m channels \u001b[39m# extract eeg channel (Channel 1) from data frame\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# ch2 = # extract emg channel (Channel 2) from data frame\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m     41\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     42\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mmax_pool2d(x, kernel_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'x' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "sched = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "train_model(model, optimizer,sched, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
